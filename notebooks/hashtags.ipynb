{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "053bc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from a CSV file and print the first 5 rows\n",
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/ame/02805_climate_conv/data/cleaned_twitter_embedded_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cd3058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweetid', 'message', 'embeddings', 'metadata', 'date', 'hashtags',\n",
      "       'location', 'sentiment', 'clean_text'],\n",
      "      dtype='object')\n",
      "51376\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Print column names\n",
    "print(data.columns)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "# Print number of rows where hashtags is null\n",
    "print(data['hashtags'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "04c743ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38402\n"
     ]
    }
   ],
   "source": [
    "# Make subset where 'clean_text' is longer than the 25% quantile and hastags is not null\n",
    "subset = data[(data['clean_text'].str.len() > data['clean_text'].str.len().quantile(0.25)) & (data['hashtags'].notnull())]\n",
    "\n",
    "# Print number of rows in subset\n",
    "print(len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e223db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g3/dglpgkv972b0cy6ntgqnpqhh0000gn/T/ipykernel_21100/1455719704.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset[\"hashtags\"] = subset[\"hashtags\"].apply(extract_tags)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_tags(x):\n",
    "    if isinstance(x, list):\n",
    "        tags = x\n",
    "    elif isinstance(x, str):\n",
    "        tags = re.findall(r\"#?\\w+\", x)\n",
    "    else:\n",
    "        tags = []\n",
    "\n",
    "    # normalize\n",
    "    return [t.lower().lstrip(\"#\") for t in tags if t.strip() != \"\"]\n",
    "    \n",
    "subset[\"hashtags\"] = subset[\"hashtags\"].apply(extract_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bf216ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique BEFORE: 29159\n"
     ]
    }
   ],
   "source": [
    "before = set(subset[\"hashtags\"].explode())\n",
    "print(\"Unique BEFORE:\", len(before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "76baea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs found: 384\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import fuzz\n",
    "from collections import defaultdict\n",
    "\n",
    "tag_counts = subset[\"hashtags\"].explode().value_counts()\n",
    "\n",
    "# Only consider tags that appear >= 5 times\n",
    "tags = tag_counts[tag_counts >= 5].index.tolist()\n",
    "\n",
    "length_buckets = defaultdict(list)\n",
    "for t in tags:\n",
    "    length_buckets[len(t)].append(t)\n",
    "\n",
    "similar_pairs = []\n",
    "\n",
    "for L, bucket in length_buckets.items():\n",
    "    candidates = (\n",
    "        bucket\n",
    "        + length_buckets.get(L-1, [])\n",
    "        + length_buckets.get(L+1, [])\n",
    "        + length_buckets.get(L-2, [])\n",
    "        + length_buckets.get(L+2, [])\n",
    "    )\n",
    "\n",
    "    for i, t1 in enumerate(bucket):\n",
    "        for t2 in candidates[i+1:]:\n",
    "            if fuzz.ratio(t1, t2) >= 92:\n",
    "                similar_pairs.append((t1, t2))\n",
    "\n",
    "print(\"Pairs found:\", len(similar_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "da1dce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters: 163\n"
     ]
    }
   ],
   "source": [
    "graph = defaultdict(set)\n",
    "for a,b in similar_pairs:\n",
    "    graph[a].add(b)\n",
    "    graph[b].add(a)\n",
    "\n",
    "def find_clusters(graph):\n",
    "    visited = set()\n",
    "    clusters = []\n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            stack=[node]\n",
    "            group=set()\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                if u not in visited:\n",
    "                    visited.add(u)\n",
    "                    group.add(u)\n",
    "                    stack.extend(graph[u])\n",
    "            clusters.append(group)\n",
    "    return clusters\n",
    "\n",
    "clusters = find_clusters(graph)\n",
    "print(\"Clusters:\", len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2cf1d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_map = {}\n",
    "\n",
    "for group in clusters:\n",
    "    canonical = tag_counts.loc[list(group)].idxmax()\n",
    "    for tag in group:\n",
    "        canonical_map[tag] = canonical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "87abcc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g3/dglpgkv972b0cy6ntgqnpqhh0000gn/T/ipykernel_21100/441880700.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset[\"hashtags\"] = subset[\"hashtags\"].apply(lambda tags: [fix_tag(t) for t in tags])\n"
     ]
    }
   ],
   "source": [
    "def fix_tag(tag):\n",
    "    return canonical_map.get(tag, tag)\n",
    "\n",
    "subset[\"hashtags\"] = subset[\"hashtags\"].apply(lambda tags: [fix_tag(t) for t in tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8827bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique AFTER: 28966\n"
     ]
    }
   ],
   "source": [
    "after = set(subset[\"hashtags\"].explode())\n",
    "print(\"Unique AFTER:\", len(after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b564a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 is the year we votethemout the year we climatestrike our hearts out the year we rebelforlife because without a liveable future nothing else matters 2020 is the year we get shit done 3 3 date 2019 12 31 location california usa sentiment negative\n",
      "Hashtags: ['votethemout', 'climatestrike', 'rebelforlife']\n",
      "\n",
      "winter has not stopped this group of dedicated climate activists they are an example to follow climatefriday climatestrike climateaction date 2019 12 27 location california usa sentiment positive\n",
      "Hashtags: ['climatefriday', 'climatestrike', 'climateaction']\n",
      "\n",
      "week 55 of climatestrike at the next week heads into its 3rd year of striking as our time on the streets gets longer we need you to act and do something for the climate in 2020 people must stop looking away and stop pretending this crisis doesn t exist at united nations date 2019 12 27 location california usa sentiment positive\n",
      "Hashtags: ['climatestrike']\n",
      "\n",
      "a year of resistance as youth protests shaped climate change discussions how active will this youth vote be in 2020 greta gretathunberg climatechange fridaysforfuture kickass climatestrike climate thwave chloegracemoretz love bhfyp date 2019 12 26 location california usa sentiment positive\n",
      "Hashtags: ['greta', 'gretathunberg', 'climatechange', 'fridaysforfuture', 'kickass', 'climatestrike', 'climate', 'thwave', 'chloegracemoretz', 'love', 'bhfyp']\n",
      "\n",
      "happy holidays greta gretathunberg climatechange fridaysforfuture kickass climatestrike chloemoretz climate thwave chloegracemoretz love globalclimatestrike bhfyp from the energy house team virginia washingtondc date 2019 12 25 location california usa sentiment positive\n",
      "Hashtags: ['greta', 'gretathunberg', 'climatechange', 'fridaysforfuture', 'kickass', 'climatestrike', 'chloemoretz', 'climate', 'thwave', 'chloegracemoretz', 'love', 'globalclimatestrike', 'bhfyp', 'virginia', 'washingtondc']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 tweets and the hashtags\n",
    "for i, row in subset.head(5).iterrows():\n",
    "    print(row['clean_text'])\n",
    "    print(\"Hashtags:\", row['hashtags'])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9600ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtags appearing more than 20 times: 804\n"
     ]
    }
   ],
   "source": [
    "# Print number of hashtags appearing more than 100 times\n",
    "from collections import Counter\n",
    "all_tags = subset[\"hashtags\"].explode()\n",
    "tag_counter = Counter(all_tags)\n",
    "popular_tags = {tag: count for tag, count in tag_counter.items() if count > 20}\n",
    "print(\"Hashtags appearing more than 20 times:\", len(popular_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "05be8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtags appearing more than 100 times: 139\n"
     ]
    }
   ],
   "source": [
    "# Print number of hashtags used more than 100 times\n",
    "print(\"Hashtags appearing more than 100 times:\", sum(1 for count in tag_counter.values() if count > 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "82db1eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hashtags before filtering: 28966\n",
      "Unique hashtags kept: 712\n",
      "Unique hashtags after filtering: 712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g3/dglpgkv972b0cy6ntgqnpqhh0000gn/T/ipykernel_21100/3317602439.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset[\"hashtags\"] = subset[\"hashtags\"].apply(filter_hashtags)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Count hashtag frequencies across all tweets\n",
    "all_tags = subset[\"hashtags\"].explode()\n",
    "tag_counts = all_tags.value_counts()\n",
    "\n",
    "# 2) Build a set of hashtags we want to KEEP\n",
    "#    - must appear at least 20 times\n",
    "#    - must appear at most 100 times\n",
    "#    - must not be exactly \"climatechange\"\n",
    "keep_tags = set(\n",
    "    tag_counts[(tag_counts >= 20) & (tag_counts <= 100)].index\n",
    ") - {\"climatechange\"}\n",
    "\n",
    "print(f\"Unique hashtags before filtering: {all_tags.nunique()}\")\n",
    "print(f\"Unique hashtags kept: {len(keep_tags)}\")\n",
    "\n",
    "# 3) Filter hashtags in each row\n",
    "def filter_hashtags(tags):\n",
    "    return [t for t in tags if t in keep_tags]\n",
    "\n",
    "subset[\"hashtags\"] = subset[\"hashtags\"].apply(filter_hashtags)\n",
    "\n",
    "# Optional: check how many tags remain\n",
    "remaining_tags = subset[\"hashtags\"].explode()\n",
    "print(f\"Unique hashtags after filtering: {remaining_tags.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2f0b16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the subset to a new CSV file\n",
    "subset.to_csv('/Users/ame/02805_climate_conv/data/cleaned_twitter_embedded_data_hashtags_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c160c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the dataframe \"message\" column for URLs and extract them into a new column \"urls\"\n",
    "import re\n",
    "def extract_urls(text):\n",
    "    url_pattern = r'(https?://\\S+)'\n",
    "    return re.findall(url_pattern, text)\n",
    "\n",
    "data['urls'] = data['message'].apply(extract_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5b1f5781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs found: 10166\n",
      "Valid URLs: 7986\n",
      "Ignored invalid URLs: 2180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling URLs: 100%|██████████| 7986/7986 [1:15:11<00:00,  1.77it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Crawl Summary =====\n",
      "Valid unique URLs:        7986\n",
      "Successfully crawled:     6480\n",
      "Failed / unreachable:     1506\n",
      "Ignored invalid URLs:     2180\n",
      "==========================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>page_content</th>\n",
       "      <th>tweet_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://00set.com</td>\n",
       "      <td>None</td>\n",
       "      <td>[37533]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://2066.uk</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class='v2' dir='ltr' la...</td>\n",
       "      <td>[33863, 33865, 36194]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://350.org,</td>\n",
       "      <td>None</td>\n",
       "      <td>[2016, 47233]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://940wfaw.com</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en-US\"&gt;\\n&lt;head&gt;\\t...</td>\n",
       "      <td>[9141]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://abcn.ws/2u93vcv</td>\n",
       "      <td>None</td>\n",
       "      <td>[24642]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      url                                       page_content  \\\n",
       "0        http://00set.com                                               None   \n",
       "1          http://2066.uk  <!DOCTYPE html>\\n<html class='v2' dir='ltr' la...   \n",
       "2         http://350.org,                                               None   \n",
       "3      http://940wfaw.com  <!DOCTYPE html>\\n<html lang=\"en-US\">\\n<head>\\t...   \n",
       "4  http://abcn.ws/2u93vcv                                               None   \n",
       "\n",
       "               tweet_ids  \n",
       "0                [37533]  \n",
       "1  [33863, 33865, 36194]  \n",
       "2          [2016, 47233]  \n",
       "3                 [9141]  \n",
       "4                [24642]  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper: validate URLs\n",
    "# --------------------------------------------------\n",
    "def clean_url(url):\n",
    "    \"\"\"Return a cleaned, valid URL or None if impossible.\"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "    \n",
    "    url = url.strip()\n",
    "\n",
    "    # Remove accidental trailing dots\n",
    "    while url.endswith(\".\"):\n",
    "        url = url[:-1]\n",
    "\n",
    "    # Add scheme if missing\n",
    "    if not url.startswith((\"http://\", \"https://\")):\n",
    "        url = \"http://\" + url\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    # Must contain at least a domain\n",
    "    if not parsed.netloc:\n",
    "        return None\n",
    "\n",
    "    # Detect double dots or malformed hostnames\n",
    "    if \"..\" in parsed.netloc:\n",
    "        return None\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper: fetch page content\n",
    "# --------------------------------------------------\n",
    "def fetch_url(url, timeout=5):\n",
    "    \"\"\"Return HTML or None if failed.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        if resp.status_code == 200:\n",
    "            return resp.text\n",
    "        return None\n",
    "    except RequestException:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Build mapping URL -> tweets\n",
    "# --------------------------------------------------\n",
    "url_to_tweets = {}\n",
    "\n",
    "for tweet_id, url_list in zip(data[\"tweetid\"], data[\"urls\"]):\n",
    "    urls = url_list if isinstance(url_list, list) else [url_list]\n",
    "\n",
    "    for u in urls:\n",
    "        cleaned = clean_url(u)\n",
    "        if cleaned:\n",
    "            url_to_tweets.setdefault(cleaned, []).append(tweet_id)\n",
    "\n",
    "# Stats\n",
    "total_urls_raw = sum(len(v if isinstance(v, list) else [v]) for v in data[\"urls\"].dropna())\n",
    "unique_valid_urls = sorted(url_to_tweets.keys())\n",
    "invalid_urls = total_urls_raw - len(unique_valid_urls)\n",
    "\n",
    "print(f\"Total URLs found: {total_urls_raw}\")\n",
    "print(f\"Valid URLs: {len(unique_valid_urls)}\")\n",
    "print(f\"Ignored invalid URLs: {invalid_urls}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Crawl valid URLs\n",
    "# --------------------------------------------------\n",
    "rows = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "for url in tqdm(unique_valid_urls, desc=\"Crawling URLs\"):\n",
    "    html = fetch_url(url)\n",
    "\n",
    "    if html is not None:\n",
    "        success_count += 1\n",
    "    else:\n",
    "        fail_count += 1\n",
    "\n",
    "    rows.append({\n",
    "        \"url\": url,\n",
    "        \"page_content\": html,\n",
    "        \"tweet_ids\": url_to_tweets[url]\n",
    "    })\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) Build dataframe\n",
    "# --------------------------------------------------\n",
    "web = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\n===== Crawl Summary =====\")\n",
    "print(f\"Valid unique URLs:        {len(unique_valid_urls)}\")\n",
    "print(f\"Successfully crawled:     {success_count}\")\n",
    "print(f\"Failed / unreachable:     {fail_count}\")\n",
    "print(f\"Ignored invalid URLs:     {invalid_urls}\")\n",
    "print(\"==========================\")\n",
    "\n",
    "web.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "65c1bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where page_content is None\n",
    "web = web[web['page_content'].notnull()]\n",
    "\n",
    "# Save web dataframe to CSV\n",
    "web.to_csv('/Users/ame/02805_climate_conv/data/crawled_web_content.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9cc1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url                                                http://2066.uk\n",
      "page_content    <!DOCTYPE html>\\n<html class='v2' dir='ltr' la...\n",
      "tweet_ids                                   [33863, 33865, 36194]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the first row of the dataframe\n",
    "print(web.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f261c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
