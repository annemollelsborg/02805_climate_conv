{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "053bc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from a CSV file and print the first 5 rows\n",
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/ame/02805_climate_conv/data/cleaned_twitter_embedded_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cd3058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweetid', 'message', 'embeddings', 'metadata', 'date', 'hashtags',\n",
      "       'location', 'sentiment', 'clean_text'],\n",
      "      dtype='object')\n",
      "51376\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Print column names\n",
    "print(data.columns)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "# Print number of rows where hashtags is null\n",
    "print(data['hashtags'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "04c743ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38402\n"
     ]
    }
   ],
   "source": [
    "# Make subset where 'clean_text' is longer than the 25% quantile and hastags is not null\n",
    "subset = data[(data['clean_text'].str.len() > data['clean_text'].str.len().quantile(0.25)) & (data['hashtags'].notnull())]\n",
    "\n",
    "# Print number of rows in subset\n",
    "print(len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e223db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g3/dglpgkv972b0cy6ntgqnpqhh0000gn/T/ipykernel_21100/1455719704.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset[\"hashtags\"] = subset[\"hashtags\"].apply(extract_tags)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_tags(x):\n",
    "    if isinstance(x, list):\n",
    "        tags = x\n",
    "    elif isinstance(x, str):\n",
    "        tags = re.findall(r\"#?\\w+\", x)\n",
    "    else:\n",
    "        tags = []\n",
    "\n",
    "    # normalize\n",
    "    return [t.lower().lstrip(\"#\") for t in tags if t.strip() != \"\"]\n",
    "    \n",
    "subset[\"hashtags\"] = subset[\"hashtags\"].apply(extract_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bf216ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique BEFORE: 29159\n"
     ]
    }
   ],
   "source": [
    "before = set(subset[\"hashtags\"].explode())\n",
    "print(\"Unique BEFORE:\", len(before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "76baea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs found: 384\n"
     ]
    }
   ],
   "source": [
    "from rapidfuzz import fuzz\n",
    "from collections import defaultdict\n",
    "\n",
    "tag_counts = subset[\"hashtags\"].explode().value_counts()\n",
    "\n",
    "# Only consider tags that appear >= 5 times\n",
    "tags = tag_counts[tag_counts >= 5].index.tolist()\n",
    "\n",
    "length_buckets = defaultdict(list)\n",
    "for t in tags:\n",
    "    length_buckets[len(t)].append(t)\n",
    "\n",
    "similar_pairs = []\n",
    "\n",
    "for L, bucket in length_buckets.items():\n",
    "    candidates = (\n",
    "        bucket\n",
    "        + length_buckets.get(L-1, [])\n",
    "        + length_buckets.get(L+1, [])\n",
    "        + length_buckets.get(L-2, [])\n",
    "        + length_buckets.get(L+2, [])\n",
    "    )\n",
    "\n",
    "    for i, t1 in enumerate(bucket):\n",
    "        for t2 in candidates[i+1:]:\n",
    "            if fuzz.ratio(t1, t2) >= 92:\n",
    "                similar_pairs.append((t1, t2))\n",
    "\n",
    "print(\"Pairs found:\", len(similar_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "da1dce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters: 163\n"
     ]
    }
   ],
   "source": [
    "graph = defaultdict(set)\n",
    "for a,b in similar_pairs:\n",
    "    graph[a].add(b)\n",
    "    graph[b].add(a)\n",
    "\n",
    "def find_clusters(graph):\n",
    "    visited = set()\n",
    "    clusters = []\n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            stack=[node]\n",
    "            group=set()\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                if u not in visited:\n",
    "                    visited.add(u)\n",
    "                    group.add(u)\n",
    "                    stack.extend(graph[u])\n",
    "            clusters.append(group)\n",
    "    return clusters\n",
    "\n",
    "clusters = find_clusters(graph)\n",
    "print(\"Clusters:\", len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2cf1d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_map = {}\n",
    "\n",
    "for group in clusters:\n",
    "    canonical = tag_counts.loc[list(group)].idxmax()\n",
    "    for tag in group:\n",
    "        canonical_map[tag] = canonical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "87abcc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g3/dglpgkv972b0cy6ntgqnpqhh0000gn/T/ipykernel_21100/441880700.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset[\"hashtags\"] = subset[\"hashtags\"].apply(lambda tags: [fix_tag(t) for t in tags])\n"
     ]
    }
   ],
   "source": [
    "def fix_tag(tag):\n",
    "    return canonical_map.get(tag, tag)\n",
    "\n",
    "subset[\"hashtags\"] = subset[\"hashtags\"].apply(lambda tags: [fix_tag(t) for t in tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8827bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique AFTER: 28966\n"
     ]
    }
   ],
   "source": [
    "after = set(subset[\"hashtags\"].explode())\n",
    "print(\"Unique AFTER:\", len(after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b564a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 is the year we votethemout the year we climatestrike our hearts out the year we rebelforlife because without a liveable future nothing else matters 2020 is the year we get shit done 3 3 date 2019 12 31 location california usa sentiment negative\n",
      "Hashtags: ['votethemout', 'climatestrike', 'rebelforlife']\n",
      "\n",
      "winter has not stopped this group of dedicated climate activists they are an example to follow climatefriday climatestrike climateaction date 2019 12 27 location california usa sentiment positive\n",
      "Hashtags: ['climatefriday', 'climatestrike', 'climateaction']\n",
      "\n",
      "week 55 of climatestrike at the next week heads into its 3rd year of striking as our time on the streets gets longer we need you to act and do something for the climate in 2020 people must stop looking away and stop pretending this crisis doesn t exist at united nations date 2019 12 27 location california usa sentiment positive\n",
      "Hashtags: ['climatestrike']\n",
      "\n",
      "a year of resistance as youth protests shaped climate change discussions how active will this youth vote be in 2020 greta gretathunberg climatechange fridaysforfuture kickass climatestrike climate thwave chloegracemoretz love bhfyp date 2019 12 26 location california usa sentiment positive\n",
      "Hashtags: ['greta', 'gretathunberg', 'climatechange', 'fridaysforfuture', 'kickass', 'climatestrike', 'climate', 'thwave', 'chloegracemoretz', 'love', 'bhfyp']\n",
      "\n",
      "happy holidays greta gretathunberg climatechange fridaysforfuture kickass climatestrike chloemoretz climate thwave chloegracemoretz love globalclimatestrike bhfyp from the energy house team virginia washingtondc date 2019 12 25 location california usa sentiment positive\n",
      "Hashtags: ['greta', 'gretathunberg', 'climatechange', 'fridaysforfuture', 'kickass', 'climatestrike', 'chloemoretz', 'climate', 'thwave', 'chloegracemoretz', 'love', 'globalclimatestrike', 'bhfyp', 'virginia', 'washingtondc']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 tweets and the hashtags\n",
    "for i, row in subset.head(5).iterrows():\n",
    "    print(row['clean_text'])\n",
    "    print(\"Hashtags:\", row['hashtags'])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9600ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtags appearing more than 20 times: 804\n"
     ]
    }
   ],
   "source": [
    "# Print number of hashtags appearing more than 100 times\n",
    "from collections import Counter\n",
    "all_tags = subset[\"hashtags\"].explode()\n",
    "tag_counter = Counter(all_tags)\n",
    "popular_tags = {tag: count for tag, count in tag_counter.items() if count > 20}\n",
    "print(\"Hashtags appearing more than 20 times:\", len(popular_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "05be8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtags appearing more than 100 times: 139\n"
     ]
    }
   ],
   "source": [
    "# Print number of hashtags used more than 100 times\n",
    "print(\"Hashtags appearing more than 100 times:\", sum(1 for count in tag_counter.values() if count > 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "82db1eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique hashtags before filtering: 28966\n",
      "Unique hashtags kept: 712\n",
      "Unique hashtags after filtering: 712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g3/dglpgkv972b0cy6ntgqnpqhh0000gn/T/ipykernel_21100/3317602439.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset[\"hashtags\"] = subset[\"hashtags\"].apply(filter_hashtags)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Count hashtag frequencies across all tweets\n",
    "all_tags = subset[\"hashtags\"].explode()\n",
    "tag_counts = all_tags.value_counts()\n",
    "\n",
    "# 2) Build a set of hashtags we want to KEEP\n",
    "#    - must appear at least 20 times\n",
    "#    - must appear at most 100 times\n",
    "#    - must not be exactly \"climatechange\"\n",
    "keep_tags = set(\n",
    "    tag_counts[(tag_counts >= 20) & (tag_counts <= 100)].index\n",
    ") - {\"climatechange\"}\n",
    "\n",
    "print(f\"Unique hashtags before filtering: {all_tags.nunique()}\")\n",
    "print(f\"Unique hashtags kept: {len(keep_tags)}\")\n",
    "\n",
    "# 3) Filter hashtags in each row\n",
    "def filter_hashtags(tags):\n",
    "    return [t for t in tags if t in keep_tags]\n",
    "\n",
    "subset[\"hashtags\"] = subset[\"hashtags\"].apply(filter_hashtags)\n",
    "\n",
    "# Optional: check how many tags remain\n",
    "remaining_tags = subset[\"hashtags\"].explode()\n",
    "print(f\"Unique hashtags after filtering: {remaining_tags.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2f0b16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the subset to a new CSV file\n",
    "subset.to_csv('/Users/ame/02805_climate_conv/data/cleaned_twitter_embedded_data_hashtags_fixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c160c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the dataframe \"message\" column for URLs and extract them into a new column \"urls\"\n",
    "import re\n",
    "def extract_urls(text):\n",
    "    url_pattern = r'(https?://\\S+)'\n",
    "    return re.findall(url_pattern, text)\n",
    "\n",
    "data['urls'] = data['message'].apply(extract_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f5781",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlenv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'url'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 1) Build mapping URL -> tweets\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[32m     55\u001b[39m url_to_tweets = {}\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tweet_id, url_list \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data[\u001b[33m\"\u001b[39m\u001b[33mtweetid\u001b[39m\u001b[33m\"\u001b[39m], \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[32m     58\u001b[39m     urls = url_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(url_list, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [url_list]\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m urls:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlenv/lib/python3.13/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlenv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'url'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper: validate URLs\n",
    "# --------------------------------------------------\n",
    "def clean_url(url):\n",
    "    \"\"\"Return a cleaned, valid URL or None if impossible.\"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "    \n",
    "    url = url.strip()\n",
    "\n",
    "    # Remove accidental trailing dots\n",
    "    while url.endswith(\".\"):\n",
    "        url = url[:-1]\n",
    "\n",
    "    # Add scheme if missing\n",
    "    if not url.startswith((\"http://\", \"https://\")):\n",
    "        url = \"http://\" + url\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    # Must contain at least a domain\n",
    "    if not parsed.netloc:\n",
    "        return None\n",
    "\n",
    "    # Detect double dots or malformed hostnames\n",
    "    if \"..\" in parsed.netloc:\n",
    "        return None\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper: fetch page content\n",
    "# --------------------------------------------------\n",
    "def fetch_url(url, timeout=5):\n",
    "    \"\"\"Return HTML or None if failed.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        if resp.status_code == 200:\n",
    "            return resp.text\n",
    "        return None\n",
    "    except RequestException:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Build mapping URL -> tweets\n",
    "# --------------------------------------------------\n",
    "url_to_tweets = {}\n",
    "\n",
    "for tweet_id, url_list in zip(data[\"tweetid\"], data[\"urls\"]):\n",
    "    urls = url_list if isinstance(url_list, list) else [url_list]\n",
    "\n",
    "    for u in urls:\n",
    "        cleaned = clean_url(u)\n",
    "        if cleaned:\n",
    "            url_to_tweets.setdefault(cleaned, []).append(tweet_id)\n",
    "\n",
    "# Stats\n",
    "total_urls_raw = sum(len(v if isinstance(v, list) else [v]) for v in data[\"url\"].dropna())\n",
    "unique_valid_urls = sorted(url_to_tweets.keys())\n",
    "invalid_urls = total_urls_raw - len(unique_valid_urls)\n",
    "\n",
    "print(f\"Total URLs found: {total_urls_raw}\")\n",
    "print(f\"Valid URLs: {len(unique_valid_urls)}\")\n",
    "print(f\"Ignored invalid URLs: {invalid_urls}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Crawl valid URLs\n",
    "# --------------------------------------------------\n",
    "rows = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "for url in tqdm(unique_valid_urls, desc=\"Crawling URLs\"):\n",
    "    html = fetch_url(url)\n",
    "\n",
    "    if html is not None:\n",
    "        success_count += 1\n",
    "    else:\n",
    "        fail_count += 1\n",
    "\n",
    "    rows.append({\n",
    "        \"url\": url,\n",
    "        \"page_content\": html,\n",
    "        \"tweet_ids\": url_to_tweets[url]\n",
    "    })\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) Build dataframe\n",
    "# --------------------------------------------------\n",
    "web = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\n===== Crawl Summary =====\")\n",
    "print(f\"Valid unique URLs:        {len(unique_valid_urls)}\")\n",
    "print(f\"Successfully crawled:     {success_count}\")\n",
    "print(f\"Failed / unreachable:     {fail_count}\")\n",
    "print(f\"Ignored invalid URLs:     {invalid_urls}\")\n",
    "print(\"==========================\")\n",
    "\n",
    "web.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1bde2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
