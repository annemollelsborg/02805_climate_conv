{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4201b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the crawled web content from CSV\n",
    "web = pd.read_csv('/Users/ame/02805_climate_conv/data/crawled_web_content.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca9e061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['url', 'page_content', 'tweet_ids'], dtype='object')\n",
      "(6480, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print column names\n",
    "print(web.columns)\n",
    "\n",
    "# Print dimensions of the DataFrame\n",
    "print(web.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769c3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import trafilatura\n",
    "\n",
    "def extract_with_newspaper(html, url=None):\n",
    "    \"\"\"Try to extract article text using newspaper3k.\"\"\"\n",
    "    try:\n",
    "        article = Article(url if url else \"\")\n",
    "        article.set_html(html)\n",
    "        article.parse()\n",
    "        text = article.text.strip()\n",
    "        if text and len(text) > 50:\n",
    "            return text\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def extract_with_trafilatura(html):\n",
    "    \"\"\"Fallback: extract text using trafilatura.\"\"\"\n",
    "    try:\n",
    "        text = trafilatura.extract(html, include_comments=False, include_tables=False)\n",
    "        if text and len(text.strip()) > 50:\n",
    "            return text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def clean_page(html, url=None):\n",
    "    \"\"\"Full cleaning pipeline: newspaper3k → trafilatura.\"\"\"\n",
    "    if not isinstance(html, str) or len(html) < 50:\n",
    "        return None\n",
    "    \n",
    "    # 1st try: newspaper3k\n",
    "    txt = extract_with_newspaper(html, url)\n",
    "    if txt:\n",
    "        return txt\n",
    "    \n",
    "    # 2nd try: trafilatura\n",
    "    txt = extract_with_trafilatura(html)\n",
    "    if txt:\n",
    "        return txt\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5add0d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting clean text: 100%|██████████| 6480/6480 [09:50<00:00, 10.98it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "clean_texts = []\n",
    "\n",
    "for html, url in tqdm(zip(web[\"page_content\"], web[\"url\"]),\n",
    "                      total=len(web),\n",
    "                      desc=\"Extracting clean text\"):\n",
    "    clean_texts.append(clean_page(html, url))\n",
    "\n",
    "web[\"clean_text\"] = clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82ba46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable cleaned pages: 2379\n",
      "Empty or unreadable pages: 4101\n"
     ]
    }
   ],
   "source": [
    "usable = web[\"clean_text\"].notna().sum()\n",
    "empty = web[\"clean_text\"].isna().sum()\n",
    "\n",
    "print(\"Usable cleaned pages:\", usable)\n",
    "print(\"Empty or unreadable pages:\", empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdf31def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://2066.uk'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first \"clean_text\"\n",
    "web[\"url\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0b45d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe as CSV\n",
    "web.to_csv('/Users/ame/02805_climate_conv/data/cleaned_web_content.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f11b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'page_content', 'tweet_ids', 'clean_text'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "314d832a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         [33863, 33865, 36194]\n",
       "1                                        [9141]\n",
       "2                                       [35230]\n",
       "3                                       [22941]\n",
       "4                                [34129, 36815]\n",
       "5                                       [34125]\n",
       "6                                       [24819]\n",
       "7                                       [23512]\n",
       "8    [16074, 16195, 23434, 23435, 23450, 23490]\n",
       "9                                       [35146]\n",
       "Name: tweet_ids, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web[\"tweet_ids\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33057166",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_web = pd.read_csv('/Users/ame/02805_climate_conv/data/cleaned_web_content.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ad23d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(x)  # turns string repr of list into real list\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "cleaned_web[\"tweet_ids\"] = cleaned_web[\"tweet_ids\"].apply(to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c523d72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 pages with the most tweet_ids:\n",
      "\n",
      "URL: http://strikewithus.org\n",
      "  Tweet IDs count: 35\n",
      "\n",
      "URL: https://spoti.fi/31be3fm\n",
      "  Tweet IDs count: 19\n",
      "\n",
      "URL: http://sealevel.climatecentral.org/maps\n",
      "  Tweet IDs count: 18\n",
      "\n",
      "URL: https://youtu.be/ehg2h9ryplq\n",
      "  Tweet IDs count: 12\n",
      "\n",
      "URL: https://eos.org/articles/dinosaurs-roar-again-now-including-a-focus-on-climate-change\n",
      "  Tweet IDs count: 11\n",
      "\n",
      "URL: http://webtv.un.org\n",
      "  Tweet IDs count: 10\n",
      "\n",
      "URL: https://neuage.org/e-books/\n",
      "  Tweet IDs count: 10\n",
      "\n",
      "URL: http://mgalleries.org\n",
      "  Tweet IDs count: 9\n",
      "\n",
      "URL: https://youtu.be/xlzfg-ir1k4\n",
      "  Tweet IDs count: 9\n",
      "\n",
      "URL: https://globalclimatestrike.net/\n",
      "  Tweet IDs count: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the length of tweet_ids for each row\n",
    "cleaned_web[\"tweet_ids_count\"] = cleaned_web[\"tweet_ids\"].apply(len)\n",
    "\n",
    "# Get the 10 pages with the most tweet_ids\n",
    "top_10 = cleaned_web.nlargest(10, 'tweet_ids_count')\n",
    "\n",
    "# Extract and print URLs\n",
    "print(\"Top 10 pages with the most tweet_ids:\\n\")\n",
    "for idx, row in top_10.iterrows():\n",
    "    print(f\"URL: {row['url']}\")\n",
    "    print(f\"  Tweet IDs count: {row['tweet_ids_count']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af7943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
